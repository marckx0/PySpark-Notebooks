{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261d7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Read_Json') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165c4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf = conf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ad7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"customerData.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "795dafcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----------------+------+\n",
      "|age|deptid|gender|             name|salary|\n",
      "+---+------+------+-----------------+------+\n",
      "| 32|   100|  male|Benjamin Garrison|  3000|\n",
      "| 40|   200|  male|    Holland Drake|  4500|\n",
      "| 26|   100|  male|  Burks Velasquez|  2700|\n",
      "| 51|   100|female|    June Rutledge|  4300|\n",
      "| 44|   200|  male|    Nielsen Knapp|  6500|\n",
      "+---+------+------+-----------------+------+\n",
      "\n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- deptid: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf = spark.read.json(inputPath)\n",
    "empDf.show()\n",
    "empDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffd32da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             name|\n",
      "+-----------------+\n",
      "|Benjamin Garrison|\n",
      "|    Holland Drake|\n",
      "|  Burks Velasquez|\n",
      "|    June Rutledge|\n",
      "|    Nielsen Knapp|\n",
      "+-----------------+\n",
      "\n",
      "+---+------+------+-------------+------+\n",
      "|age|deptid|gender|         name|salary|\n",
      "+---+------+------+-------------+------+\n",
      "| 40|   200|  male|Holland Drake|  4500|\n",
      "+---+------+------+-------------+------+\n",
      "\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|female|    1|\n",
      "|  male|    4|\n",
      "+------+-----+\n",
      "\n",
      "+------+------------------+--------+\n",
      "|deptid|       avg(salary)|max(age)|\n",
      "+------+------------------+--------+\n",
      "|   200|            5500.0|      44|\n",
      "|   100|3333.3333333333335|      51|\n",
      "+------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.select(\"name\").show()\n",
    "empDf.filter(empDf[\"age\"] == 40).show()\n",
    "empDf.groupBy(\"gender\").count().show()\n",
    "empDf.groupBy(\"deptid\").agg({\"salary\": \"avg\",\"age\":\"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3d188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|100|      Sales|\n",
      "|200|Engineering|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#crea un marco de datos de una lista\n",
    "deptList = [{'name': 'Sales', 'id':\"100\"},\\\n",
    "           {'name':'Engineering', 'id':\"200\"}]\n",
    "deptDf = spark.createDataFrame(deptList)\n",
    "deptDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994de9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----------------+------+---+-----------+\n",
      "|age|deptid|gender|             name|salary| id|       name|\n",
      "+---+------+------+-----------------+------+---+-----------+\n",
      "| 51|   100|female|    June Rutledge|  4300|100|      Sales|\n",
      "| 26|   100|  male|  Burks Velasquez|  2700|100|      Sales|\n",
      "| 32|   100|  male|Benjamin Garrison|  3000|100|      Sales|\n",
      "| 44|   200|  male|    Nielsen Knapp|  6500|200|Engineering|\n",
      "| 40|   200|  male|    Holland Drake|  4500|200|Engineering|\n",
      "+---+------+------+-----------------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#unir data frames\n",
    "empDf.join(deptDf,empDf.deptid == deptDf.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8719eda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+\n",
      "|deptid|avg(salary)|max(age)|\n",
      "+------+-----------+--------+\n",
      "|   200|     5500.0|      44|\n",
      "|   100|     3650.0|      51|\n",
      "+------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Operaciones en cascada\n",
    "empDf.filter(empDf[\"age\"] > 30).join(deptDf, empDf.deptid == deptDf.id).\\\n",
    "    groupBy(\"deptid\").\\\n",
    "    agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dacc1c",
   "metadata": {},
   "source": [
    "#### Crear dataframes a partir de RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00186334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "lines = sc.textFile(\"auto-data.csv\")\n",
    "#quitar la primera linea\n",
    "datalines = lines.filter(lambda x: \"FUELTYPE\" not in x)\n",
    "datalines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51445776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     body| hp|      make|\n",
      "+---------+---+----------+\n",
      "|hatchback| 69|    subaru|\n",
      "|hatchback| 48| chevrolet|\n",
      "|hatchback| 68|     mazda|\n",
      "|hatchback| 62|    toyota|\n",
      "|hatchback| 68|mitsubishi|\n",
      "|hatchback| 60|     honda|\n",
      "|    sedan| 69|    nissan|\n",
      "|hatchback| 68|     dodge|\n",
      "|hatchback| 68|  plymouth|\n",
      "|hatchback| 68|     mazda|\n",
      "|hatchback| 68|mitsubishi|\n",
      "|hatchback| 68|     dodge|\n",
      "|hatchback| 68|  plymouth|\n",
      "|hatchback| 70| chevrolet|\n",
      "|hatchback| 62|    toyota|\n",
      "|hatchback| 68|     dodge|\n",
      "|hatchback| 58|     honda|\n",
      "|hatchback| 62|    toyota|\n",
      "|hatchback| 76|     honda|\n",
      "|    sedan| 70| chevrolet|\n",
      "+---------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parts = datalines.map(lambda l: l.split(\",\"))\n",
    "autoMap = parts.map(lambda p: Row(make=p[0],\\\n",
    "                                 body=p[4],\n",
    "                                 hp=int(p[7])))\n",
    "#inferir el esquema y registrar el dataframe como una tabla\n",
    "autoDf = spark.createDataFrame(autoMap)\n",
    "autoDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2943ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
